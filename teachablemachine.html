<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Teachable Machine Project</title>
  <link rel="stylesheet" href="stylepage.css">
</head>

<body>

  <!-- Header -->
  <header class="page-header">
    <h1>Teachable Machine Project</h1>
    <h2>Code the Future. Include Everyone.</h2>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="aboutus.html">About Us</a></li>
        <li><a href="resources.html">Resources</a></li>
        <li><a href="techhero.html">Tech Hero</a></li>
        <li><a href="teachablemachine.html" class="active">Teachable Machine</a></li>
      </ul>
    </nav>
  </header>



  <main>

    <!-- Project Statement -->
    <section id="project-statement" class="content-section">
      <h2>Project Statement: Daily Object Image Classification</h2>

      <p>
        For this project, we created an image-based machine learning model using Google’s Teachable Machine and embedded it
        into our group website. The main goal was to understand how image classification works—from collecting data, to
        training the model, to deploying it on a webpage that users can interact with. This project allowed us to combine
        machine learning concepts with front-end web design while also thinking critically about how AI models behave in
        real environments.
      </p>

      <p>
        We chose to classify four everyday objects that we commonly use:
        <strong>a book, a Labubu figure, a water bottle, and a tissue</strong>.
        These objects were visually distinct and easy to capture through the webcam, making them ideal examples for an
        introductory image classification project. We intentionally selected items with different shapes, textures, and 
        colors to see whether the model could learn to differentiate between them.
      </p>

      <p>
        To build the dataset, we captured multiple images of each object using a laptop webcam. For every category, we 
        experimented with various angles, distances, and lighting conditions to help the model learn more generalized 
        patterns. For example, we rotated the book and water bottle, held the Labubu figure at different heights, and 
        positioned the tissue in both folded and unfolded shapes. This process demonstrated how essential variation is when 
        creating a robust machine learning classifier.
      </p>

      <p>
        After training the model, we used the live preview tool in Teachable Machine to test accuracy. We noticed that 
        lighting played a major role in classification performance. Under bright, even lighting, the model predicted 
        correctly most of the time. However, when shadows appeared or when we moved objects too quickly, the model hesitated 
        or made mistakes. This showed us that machine learning is highly sensitive to training conditions and that real-world 
        performance can differ significantly from controlled training scenarios.
      </p>

      <p>
        Once we were satisfied with the model’s performance, we exported it as a “web model” and integrated the JavaScript 
        code into our <code>teachablemachine.html</code> page. We also recorded a demonstration video to explain how the 
        classifier works, how each object was trained, and how users can interact with the live camera feed. This allowed us 
        to present the project in a more interactive and accessible format.
      </p>

      <p>
        Overall, this project strengthened our understanding of how data collection, testing, and web development intersect 
        in machine learning. We saw firsthand how training data quality affects accuracy and how small choices—such as 
        background, lighting, and object positioning—shape what the model learns. This experience helped us appreciate the 
        complexity behind AI systems that appear simple on the surface.
      </p>
    </section>




    <!-- Reflection Section -->
    <section id="reflection" class="content-section">
      <h2>Reflection on Joy Buolamwini’s <em>Unmasking AI</em></h2>

      <p>
        Working on our image classification project helped us understand Joy Buolamwini’s
        <em>Unmasking AI</em> in a much deeper and more personal way. Joy writes that
        <strong>“the architecture of AI is shaped by the priorities and prejudices of those who build it”</strong>,
        and we saw this play out as we trained our model. Since our dataset consisted entirely of our own images of a book,
        a Labubu figure, a water bottle, and a tissue, the model learned only from the conditions we provided—our lighting,
        our angles, our background, and our choices. This made us realize that even small AI projects reflect the 
        limitations of their creators.
      </p>

      <p>
        Joy’s concept of the <strong>coded gaze</strong> became especially relevant as we tested our classifier.
        She defines this as the bias embedded in systems shaped by developers’ assumptions. 
        Our classifier performed best when the objects were shown in lighting or positions similar to the training images. 
        When shadows appeared or when we held the Labubu figure too far from the camera, the model sometimes misclassified 
        it. This helped us understand how large-scale AI systems struggle when encountering users or conditions that differ 
        from the “norms” present in their datasets.
      </p>

      <p>
        Another part of Joy’s book that resonated with us was her discussion about datasets created without consent.
        She notes that many AI systems are trained using images <strong>“scraped from the internet, taken without 
        permission from the people depicted.”</strong> Even though we built a small model using only our own objects,
        the book reminded us how easy it is for developers to overlook data ethics when collecting and labeling images.
      </p>

      <p>
        Joy also writes, <strong>“When AI fails, it doesn’t fail equally.”</strong> While the mistakes in our project 
        were harmless—such as misreading a tissue as a water bottle—the same kind of errors in policing, hiring, or 
        identity verification could cause harm. This helped us understand why accuracy is not simply a technical goal, 
        but also a social and ethical one.
      </p>

      <p>
        Through this assignment, we realized that building an AI model is not just a technical task but an ethical 
        responsibility. Joy’s message—that we can demand better and design more inclusive technology—encourages us to
        think critically about every choice we made: the objects we selected, the lighting we used, the images we collected,
        and the way we presented the results. This project made us more aware of the responsibilities that come with 
        designing AI, even in a classroom setting.
      </p>
    </section>




    <!-- Demo Video -->
    <section id="demo-video" class="content-section">
      <h2>Demo Videos: Teachable Machine in Action</h2>
      <p>Watch our demonstration videos below to see how the image classifier works in real time.</p>

  <!-- First video -->
      <div class="video-container">
        <iframe
          src="https://youtu.be/Qfr3LmeOUhg"
          title="Teachable Machine Demo 1"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen>
        </iframe>
      </div>

      <br><br>

  <!-- Second video -->
      <div class="video-container">
        <iframe
          src="https://youtu.be/YzIQh9GWt4w"
          title="Teachable Machine Demo 2"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen>
        </iframe>
      </div>
    </section>

    <section id="tm-demo" class="content-section">
      <h2>Live Teachable Machine Classifier</h2>
      <p>Turn on your webcam (if you're comfortable) and test the model live below.</p>

      <!-- This div is where the p5 canvas will appear -->
      <div id="p5-container"></div>

      <!-- Libraries -->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/p5.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.9.0/addons/p5.dom.min.js"></script>
      <script src="https://unpkg.com/ml5@0.12.2/dist/ml5.min.js"></script>
      
  <!-- Your Teachable Machine + p5.js code -->
      <script>
        // Video
        let video;
        // Displaying label
        let label = "waiting...";
        // The classifier
        let classifier;

        // Teachable Machine hosted model URL
        let modelURL = "https://teachablemachine.withgoogle.com/models/1jojGvoQ0/";
    
        // STEP 1: Load the model!
        function preload() {
          classifier = ml5.imageClassifier(modelURL + "model.json");
        }

        function setup() {
          // Create the canvas and attach it to the p5-container div
          const canvas = createCanvas(640, 520);
          canvas.parent("p5-container");
    
          // Create the video
          video = createCapture(VIDEO);
          video.size(640, 480);
          video.hide();
    
          // STEP 2: Start classifying
          classifyVideo();
        }

          // STEP 3 classify!
        function classifyVideo() {
          classifier.classify(video, gotResults);
        }
      
        function draw() {
          background(0);
      
          // Draw the video
          image(video, 0, 0);
      
          // Draw the label
          textSize(32);
          textAlign(CENTER, CENTER);
          fill(255);
          text(label, width / 2, height - 16);
        }

        // STEP 4: Get the classification!
        function gotResults(error, results) {
          // Something went wrong!
          if (error) {
            console.error(error);
            return;
          }
          // Store the label and classify again!
          label = results[0].label;
          classifyVideo();
        }
      </script>
    </section>



  </main>



  
