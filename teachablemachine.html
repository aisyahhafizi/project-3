<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Teachable Machine Project</title>
  <link rel="stylesheet" href="stylepage.css">
</head>

<body>

  <!-- Header -->
  <header class="page-header">
    <h1>Teachable Machine Project</h1>
    <h2>Code the Future. Include Everyone.</h2>
    <nav>
      <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="aboutus.html">About Us</a></li>
        <li><a href="resources.html">Resources</a></li>
        <li><a href="techhero.html">Tech Hero</a></li>
        <li><a href="teachablemachine.html" class="active">Teachable Machine</a></li>
        <li><a href="tryourmodel.html">Try Our Model</a></li>
      </ul>
    </nav>
  </header>



  <main>

    <!-- Aisyah's Project Statement  -->
    <section id="project-statement-aisyah" class="content-section">
      <h2>Project Statement – Aisyah Sofea Hafizi</h2>
    
      <p>
        For this project, we created an image-based machine learning model using Google’s Teachable Machine and embedded it
        into our group website. The main goal was to understand how image classification works, from collecting data, to
        training the model, to deploying it on a webpage that users can interact with. This project allowed us to combine
        machine learning concepts with front-end web design while also thinking critically about how AI models behave in
        real environments.
      </p>
    
      <p>
        We chose to classify four everyday objects that we commonly use:
        <strong>a book, a Labubu figure, a water bottle and a tissue box</strong>.
        These objects were visually distinct and easy to capture through the webcam, making them ideal examples for an
        introductory image classification project. We intentionally selected items with different shapes, textures and 
        colors to see whether the model could learn to differentiate between them.
      </p>

      <p>
        To build the dataset, we captured multiple images of each object using a laptop webcam. For every category, we 
        experimented with various angles and distances to help the model learn more generalized 
        patterns. For example, we rotated the book and water bottle, held the Labubu figure at different heights and 
        positioned the tissue box from the back and front. This process demonstrated how essential variation is when 
        creating a robust machine learning classifier.
      </p>
    
      <p>
        After training the model, we used the live preview tool in Teachable Machine to test accuracy. We noticed that 
        distance played a major role in classification performance. The nearer the object to the webcam, the more the model 
        predicted correctly most of the time. However, when we put the object far from the webcam or when we moved objects 
        too quickly, the model hesitated or made mistakes. This showed us that machine learning is highly sensitive to 
        training conditions and that real-world performance can differ significantly from controlled training scenarios.
      </p>
    
      <p>
        Once we were satisfied with the model’s performance, we exported it as a “web model” and integrated the JavaScript 
        code into our <code>teachablemachine.html</code> page. We also recorded a demonstration video to explain how the 
        classifier works, how each object was trained and how users can interact with the live camera feed. This allowed us 
        to present the project in a more interactive and accessible format.
      </p>

      <p>
        Overall, this project strengthened our understanding of how data collection, testing, and web development intersect 
        in machine learning. We saw firsthand how training data quality affects accuracy and how small choices such as 
        background, lighting and object positioning shape what the model learns. This experience helped us appreciate the 
        complexity behind AI systems that appear simple on the surface.
      </p>
    </section>

    <!-- Aisyah's Reflection -->
    <section id="reflection-aisyah" class="content-section">
      <h2>Reflection on <em>Unmasking AI</em> – Aisyah Sofea Hafizi</h2>
    
      <p>
        Working on our image classification project helped us understand Joy Buolamwini’s
        <em>Unmasking AI</em> in a much deeper and more personal way. Joy writes that
        <strong>“the architecture of AI is shaped by the priorities and prejudices of those who build it”</strong>,
        and we saw this play out as we trained our model. Since our dataset consisted entirely of our own images of a book,
        a Labubu figure, a water bottle and a tissue box, the model learned only from the conditions we provided such as our 
        distances, our angles, our background and our choices. This made us realize that even small AI projects reflect the 
        limitations of their creators.
      </p>
    
      <p>
        Joy’s concept of the <strong>coded gaze</strong> became especially relevant as we tested our classifier.
        She defines this as the bias embedded in systems shaped by developers’ assumptions. 
        Our classifier performed best when the objects were shown in distances, positions or lightings similar to the training images. 
        When shadows appeared or when we held the Labubu figure too far from the camera, the model sometimes misclassified 
        it. This helped us understand how large-scale AI systems struggle when encountering users or conditions that differ 
        from the “norms” present in their datasets.
      </p>

      <p>
        Another part of Joy’s book that resonated with us was her discussion about datasets created without consent.
        She notes that many AI systems are trained using images <strong>“scraped from the internet, taken without 
        permission from the people depicted.”</strong> Even though we built a small model using only our own objects,
        the book reminded us how easy it is for developers to overlook data ethics when collecting and labeling images.
      </p>

      <p>
        Joy also writes, <strong>“When AI fails, it doesn’t fail equally.”</strong> While the mistakes in our project 
        were harmless—such as misreading a tissue as a water bottle—the same kind of errors in policing, hiring or 
        identity verification could cause harm. This helped us understand why accuracy is not simply a technical goal, 
        but also a social and ethical one.
      </p>
    
      <p>
        Through this assignment, we realized that building an AI model is not just a technical task but an ethical 
        responsibility. Joy’s message that we can demand better and design more inclusive technology encourages us to
        think critically about every choice we made: the objects we selected, the lighting we used, the images we collected
        and the way we presented the results. This project made us more aware of the responsibilities that come with 
        designing AI, even in a classroom setting.
      </p>
    </section>

    <!-- Maia's Project Statement  -->
    <section id="project-statement-maia" class="content-section">
      <h2>Project Statement – Maia Nebo</h2>
    
      <p>
        In this project, we implemented an image analyzing machine learning model into our website using Teachable Machine’s on Google.
        This project helped us gain an understanding of how algorithms, image processing, and machine learning work.
        Creating this model took us through step-by-step on image analysis and data feedback loops. This project became a lesson
        in ethics as well as Buolamwini’s Unmasking AI, shaping expectations about algorithmic bias.
      </p>
    
      <p>
        The main goal of this teachable machine was to be able to classify everyday objects with minimal error and efficiency.
        The objects that were chosen were four very distinct objects with different size and texture, so we could see if the model
        could observe and distinguish effectively. For our items, we chose a book, a Labubu, a water bottle, and a tissue box.
        We chose simple household objects because we wanted to focus on understanding bias rather than on complexity.
      </p>

      <p>
        Google's teachable machine is a web based tool that allows us to create our machine learning model while being accessible
        and straightforward. We collected our images using a laptop webcam and grouped them together. We grouped by object, but
        there were many positions/angles for every object to ensure a full-bodied machine. It was important to capture these
        objects from different angles so that we could minimize limitations. Buolamwini puts forth the idea of limitations,
        with many data sets lacking representation even on a large scale. Our data almost mirrors that of a smaller scale because
        we were limited on photography elements like camera quality, background, and lighting situations.
      </p>
    
      <p>
        After we had uploaded the images at angles, we trained the model using the train model button and the live preview tool
        on the website. The train model platform on Teachable machines uses the data/images uploaded and processes patterns
        within it to help the machine differentiate and classify. The main issue that we had occurred was a slight problem
        with the depth perception of the model. When the object was placed far it made a lot more mistakes and had more
        trouble than when the object was placed closer. It’s almost as if the machine could not gauge how far the object was
        and what group it actually belonged to. The model may have had faults at first due to a lack of data or an environmental
        mismatch. To us, it was an example of the myth of AI neutrality because the machine's effectiveness was all reliant
        on training and the data that we chose to use. The platform did not take long to train the model, but we noticed how
        the website is not transparent on how it weighted different features.
      </p>

      <p>
        After the testing of our model, we implemented it into our website's .html page and also uploaded the demonstration video.
        This model ended up being both efficient while also being accessible. Ultimately, this project helped us understand how
        machine learning works; it also helped us understand how limited datasets, quality, and misclassifications could cause
        harm when used at a large scale. We were only allowed to use up to 30 images for each object on teachable machines,
        if a small dataset was used like that in a larger machine learning model, it could add many disadvantages to users.
      </p>
    </section>


   <!-- Maia's Reflection  -->
    <section id="reflection-maia" class="content-section">
      <h2>Reflection on <em>Unmasking AI</em> – Maia Nebo</h2>
    
      <p>
        This project helped us understand Joy Buoloamwini’s Unmasking AI further through a real world example that we created
        ourselves. Her themes of data inequality, need for transparency, and the myth of AI neutrality really stuck throughout
        the project. As we tested the project, the coded gaze really stuck out when the model performed better under the lighting
        conditions that were set originally. As stated previously, we also felt as if the Teachable Machines website itself
        failed to provide transparency on what features were being analyzed to distinguish different objects from each other.
        Buolawmi emphasizes the importance of transparency; without transparency, there can be no meaningful direction and
        oversight.
      </p>
    
      <p>
        Buoloamwini writes, “When companies require individuals to fit a narrow definition of acceptable behavior encoded into
        a machine learning model, they will reproduce harmful patterns of exclusion and suspicion.” Teachable machines only
        allowed us to upload 30 images to train our model, but what if our objects had more extensive features? Using narrow
        datasets like the one we used on our machines can lack representation that could disadvantage users. This helped us
        understand how AI systems can exemplify bias.
      </p>

      <p>
        Another lesson that resonated with us from this project was the idea that bias in AI systems is systemic and that
        there is a large myth of AI neutrality. Buolamwini also writes, “Default settings are not neutral. They often reflect
        the coded gaze — the preferences of those who have the power to choose what subjects to focus on.” Inequalities in
        outcomes of the data in these sets are not due to a flaw in the code; they are due to flaws in the datasets and
        assumptions by the individuals who are creating these machines.
      </p>
    
      <p>
        Finally Unmasking AI highlights the importance of listening to marginalized voices and growing diversity in the future
        of AI development. Our small dataset was an example of how exclusion and inaccessibility can occur. What happens when
        these narrow datasets are used for large corporations which reflect biases? It also made us realize how entire groups
        may be misrepresented because they were never included in the data decision process to begin with. This assignment
        helped us appreciate Buolamwini’s push for inclusivity in coding and system design that takes in many perspectives.
      </p>

      <p>
        Overall, this project allowed our group to understand that the issues in Joy Buolamwini’s Unmasking AI are not small
        and distant issues. Issues like dataset inequality, algorithmic injustice, and the need for transparency can be shown
        in small tools like Teachable Machines. This project helped us understand that biases can be shown long before a
        system is widely accessible to people. This project also helped us reinforce the idea that AI and machine learning
        models carry important ethical responsibilities that must be addressed during the creation rather than treated as
        problems to patch later, after users have been negatively impacted.
      </p>
    </section>


   <!-- Aveyna's Project Statement  -->
    <section id="aveyna-combined" class="content-section">
      <h2>Project Statement & Reflection – Aveyna Mao</h2>
    
      <p>
        Creating our machine model while reading <em>Unmasking AI</em> gave us hands-on experience with the issues Buolamwini raises 
        about bias, the coded gaze, and the social consequences of algorithmic systems. One of the most resonant lessons from 
        <em>Unmasking AI</em> is the idea that discrimination is often embedded long before a model ever makes a prediction. 
        Buolamwini’s story about the white mask echoed through our work. When she wrote, “The software did not see me,” it forced us 
        to question whom our model would fail to see, as well. Even in this controlled assignment, we confronted the discomfort of 
        producing a tool that “sees” some people more clearly than others.
      </p>
    
      <p>
        S. Craig Watkins’ chapter, “Digital Gates,” helped us frame these issues beyond race and into aesthetics and cultural value. 
        Facebook’s “clean” interface and MySpace’s “chaotic” reputation were not just about design; they were reflections of which 
        users were deemed respectable or desirable. Our classifier implicitly defined what kinds of inputs were “normal,” those 
        resembling the ones we trained. Everyone else faced a digital gate of reduced accuracy.
      </p>

      <p>
        Our class discussions about AI as a tool also resonated deeply. The Deloitte scandal recently, where generative AI pulled 
        inaccurate sources, reminded us why AI should enhance human judgment rather than replace it. Our model reinforced this lesson. 
        We could see its potential, but also its limits. It needed human oversight, contextual knowledge, and ethical consideration 
        to be used responsibly.
      </p>
    
      <p>
        Intersectionality added another layer. Bias isn’t simply additive — it compounds. A classifier built with insufficient diversity 
        might fail especially for those whose identities already rest at multiple marginalized intersections; people whose features, 
        accents, or cultural expressions the dataset never captured.
      </p>
    
      <p>
        By the end of the assignment, we better understood the structural reasons why AI systems replicate inequality. It’s not because 
        engineers explicitly choose discrimination but because they design within frameworks that reward convenience, speed, and 
        perceived neutrality. Our project reveals how easy it is to reproduce these dynamics.
      </p>
    
      <p>
        Overall, the assignment made us more critically aware. We learned that building an algorithm requires humility, reflection, and 
        equity — not just technical skill. The lessons from <em>Unmasking AI</em> and other class readings will continue to guide how we 
        think about AI. Not as an inevitable replacement for human judgment, but as a tool that must be intentionally shaped to avoid 
        perpetuating harm.
      </p>
    </section>
</body>
</html>


  
